{
  "llm_config": {
    "config_list": [
      {
        "model": "llama3.2:latest",
        "api_base": "http://localhost:11434/v1",
        "api_key": "ollama"
      }
    ],
    "temperature": 0.1,
    "timeout": 120
  }
}